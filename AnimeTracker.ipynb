from google.colab import drive
drive.mount('/content/drive')

!pip install requests beautifulsoup4 gspread oauth2client

import requests
import time
import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import re

scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name("/content/drive/MyDrive/Perso/TBATE/Web Anime/credentials.json", scope)
client = gspread.authorize(creds)

def search_anime_by_title(title):
    url = f"https://api.jikan.moe/v4/anime?q={title}&limit=1"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            if data["data"]:
                anime = data["data"][0]
                anime_id = anime["mal_id"]
                Image = anime["images"]["jpg"]["image_url"]
                nb_episode = anime.get("episodes")  # parfois None si pas encore annoncé
                return anime_id,  Image, nb_episode
            else:
                print("Aucun anime trouvé.")
                return None
        else:
            print(f"Erreur lors de la requête : {response.status_code}")
            return None
    except Exception as e:
        print(f"Une erreur est survenue : {e}")
        return None

def is_season_indicator(title):
    return bool(re.search(r'(saison|s)\s?\d+$', title, re.IGNORECASE)) or bool(re.search(r'\s\d+$', title))

def simplify_title(title):
    if is_season_indicator(title):
        return re.sub(r'(\s+(saison|s)?\s?\d+)$', '', title, flags=re.IGNORECASE).strip()
    return title.strip()

def get_slug_from_title(title):
    title = simplify_title(title)
    search_url = f"https://v6.voiranime.com/?s={title}"  # URL de recherche
    response = requests.get(search_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        anime_links = soup.find_all("a", href=True)
        for link in anime_links:
            if '/anime/' in link['href']:  # Vérifier si le lien est lié à un anime
                slug = link['href'].split('/anime/')[1].split('/')[0]  # Extraire le slug
                if slug.endswith("-vf"):
                   slug = slug[:-3]
                search_url = "https://v6.voiranime.com/anime/" + slug + "/"  # URL de recherche
                response = requests.get(search_url)
                soup = BeautifulSoup(response.text, "html.parser")
                Dispo = re.search(r'\b(\d{2})\b', soup.find("li", class_="wp-manga-chapter").find("a").text.strip()).group(1)  # Nombre d'épisodes dispo
                return slug, Dispo
    print("Error get slug from title")
    return None

def update_google_sheet(Titre, Vus, Dispo, nb_episode, ID_MAL, Slug, Image, worksheet):
    found = False
    records = worksheet.get_all_records()
    for i, row in enumerate(records):
        if row['Titre'] == Titre:
          row_index = i + 2
          worksheet.update(range_name=(f"A{row_index}:G{row_index}"), values=[[Titre, Vus, Dispo, nb_episode, ID_MAL, Slug, Image]])
          found = True
          break
    if not found:
      worksheet.append_row([Titre, Vus, Dispo, nb_episode, ID_MAL, Slug, Image]) 

def get_data(titles, records):
    for i, title in enumerate(titles):
      Titre = title
      ID_MAL, Image, nb_episode = search_anime_by_title(title)
      Slug, Dispo = get_slug_from_title(title)
      if Slug == "to-be-hero-x-jap":
        Slug = "to-be-hero-x"
      Vus = 0
      for i, row in enumerate(records):
          if row['Titre'] == Titre:
              Vus = row['Vus']
              if Vus == "":
                Vus = 0
      update_google_sheet(Titre, Vus, Dispo, nb_episode, ID_MAL, Slug, Image, worksheet)
    return None

if __name__ == "__main__":
    spreadsheet = client.open("Scan")
    worksheet = spreadsheet.worksheet("Anime")
    records = worksheet.get_all_records()
    titles = [record['Titre'] for record in records]
    get_data(titles, records)
    print("Anime ajouté avec succès à Google Sheets !")
